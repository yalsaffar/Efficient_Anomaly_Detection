{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The data used is the anolamly detection database called \"Numenta Anomaly Benchmark\" (NAB) which  is a novel benchmark for evaluating algorithms for anomaly detection in streaming, real-time applications. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score # for evaluation only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory path where the CSV files are stored\n",
    "base_dir = 'data/numenta NAB master data/'\n",
    "\n",
    "# List to store tuples of (file name, DataFrame)\n",
    "dfs = []\n",
    "\n",
    "# Walk through all subdirectories and files\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):  # Check if the file is a CSV\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(root, file)\n",
    "            # Read the CSV file and store it in a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Append the tuple (file name, DataFrame) to the list\n",
    "            dfs.append((file_path.split('/')[-1].replace('\\\\', '/'), df))\n",
    "\n",
    "# Now `dfs` contains tuples of (file name, DataFrame) from all CSV files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame for artificialWithAnomaly/art_daily_flatmiddle.csv:\n",
      "            timestamp      value  anomaly\n",
      "0 2014-04-01 00:00:00 -21.048383        0\n",
      "1 2014-04-01 00:05:00 -20.295477        0\n",
      "2 2014-04-01 00:10:00 -18.127229        0\n",
      "3 2014-04-01 00:15:00 -20.171665        0\n",
      "4 2014-04-01 00:20:00 -21.223762        0\n",
      "Updated DataFrame for artificialWithAnomaly/art_daily_jumpsdown.csv:\n",
      "            timestamp      value  anomaly\n",
      "0 2014-04-01 00:00:00  18.090486        0\n",
      "1 2014-04-01 00:05:00  20.359843        0\n",
      "2 2014-04-01 00:10:00  21.105470        0\n",
      "3 2014-04-01 00:15:00  21.151585        0\n",
      "4 2014-04-01 00:20:00  18.137141        0\n",
      "Updated DataFrame for artificialWithAnomaly/art_daily_jumpsup.csv:\n",
      "            timestamp      value  anomaly\n",
      "0 2014-04-01 00:00:00  19.761252        0\n",
      "1 2014-04-01 00:05:00  20.500833        0\n",
      "2 2014-04-01 00:10:00  19.961641        0\n",
      "3 2014-04-01 00:15:00  21.490266        0\n",
      "4 2014-04-01 00:20:00  20.187739        0\n",
      "Updated DataFrame for artificialWithAnomaly/art_daily_nojump.csv:\n",
      "            timestamp      value  anomaly\n",
      "0 2014-04-01 00:00:00  21.598011        0\n",
      "1 2014-04-01 00:05:00  19.321392        0\n",
      "2 2014-04-01 00:10:00  21.399938        0\n",
      "3 2014-04-01 00:15:00  18.373916        0\n",
      "4 2014-04-01 00:20:00  18.167499        0\n",
      "Updated DataFrame for artificialWithAnomaly/art_increase_spike_density.csv:\n",
      "            timestamp  value  anomaly\n",
      "0 2014-04-01 00:00:00   20.0        0\n",
      "1 2014-04-01 00:05:00    0.0        0\n",
      "2 2014-04-01 00:10:00    0.0        0\n",
      "3 2014-04-01 00:15:00    0.0        0\n",
      "4 2014-04-01 00:20:00    0.0        0\n",
      "Updated DataFrame for artificialWithAnomaly/art_load_balancer_spikes.csv:\n",
      "            timestamp  value  anomaly\n",
      "0 2014-04-01 00:00:00    0.0        0\n",
      "1 2014-04-01 00:05:00    0.0        0\n",
      "2 2014-04-01 00:10:00    0.0        0\n",
      "3 2014-04-01 00:15:00    0.0        0\n",
      "4 2014-04-01 00:20:00    0.0        0\n",
      "Updated DataFrame for realKnownCause/ambient_temperature_system_failure.csv:\n",
      "            timestamp      value  anomaly\n",
      "0 2013-07-04 00:00:00  69.880835        0\n",
      "1 2013-07-04 01:00:00  71.220227        0\n",
      "2 2013-07-04 02:00:00  70.877805        0\n",
      "3 2013-07-04 03:00:00  68.959400        0\n",
      "4 2013-07-04 04:00:00  69.283551        0\n",
      "Updated DataFrame for realKnownCause/cpu_utilization_asg_misconfiguration.csv:\n",
      "            timestamp   value  anomaly\n",
      "0 2014-05-14 01:14:00  85.835        0\n",
      "1 2014-05-14 01:19:00  88.167        0\n",
      "2 2014-05-14 01:24:00  44.595        0\n",
      "3 2014-05-14 01:29:00  56.282        0\n",
      "4 2014-05-14 01:34:00  36.534        0\n",
      "Updated DataFrame for realKnownCause/rogue_agent_key_hold.csv:\n",
      "            timestamp     value  anomaly\n",
      "0 2014-07-06 20:10:00  0.064535        0\n",
      "1 2014-07-06 20:15:00  0.064295        0\n",
      "2 2014-07-06 20:20:00  0.063880        0\n",
      "3 2014-07-06 20:25:00  0.065692        0\n",
      "4 2014-07-06 20:35:00  0.056301        0\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "with open('data/combined_labels.json', 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Iterate over the list of (file name, DataFrame) tuples\n",
    "for file_name, df in dfs:\n",
    "    # Get the corresponding file name's anomalies from the JSON file\n",
    "    anomaly_times = labels.get(file_name, [])\n",
    "\n",
    "    # Convert the DataFrame's time column to datetime if not already\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    else:\n",
    "        raise ValueError(f\"DataFrame for {file_name} does not have a 'timestamp' column.\")\n",
    "\n",
    "    # Create the 'anomaly' column and mark anomalies based on the timestamps in the JSON\n",
    "    df['anomaly'] = df['timestamp'].apply(lambda x: 1 if str(x) in anomaly_times else 0)\n",
    "\n",
    "    # If you'd like to print or check the updated DataFrame\n",
    "    print(f\"Updated DataFrame for {file_name}:\")\n",
    "    print(df.head())  # Print the first few rows to confirm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I have read and researched many algorithms, However the following were choosen because of their efficent implementation using only Numpy, and their diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OMLADStreaming:\n",
    "    def __init__(self, threshold=3):\n",
    "        \"\"\"\n",
    "        Streaming anomaly detection using a rolling Z-score.\n",
    "\n",
    "        Parameters:\n",
    "        threshold (float): Z-score threshold to classify anomalies.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.n = 0  # Counter for the number of observations\n",
    "        self.mean = 0  # Initial mean\n",
    "        self.m2 = 0  # Sum of squares of differences from the mean\n",
    "        self.std = 0  # Initial standard deviation\n",
    "        self.anomalies = []  # To store anomaly detection results\n",
    "\n",
    "    def update_stats(self, new_value):\n",
    "        \"\"\"\n",
    "        Update the rolling mean and standard deviation with a new value.\n",
    "        \n",
    "        Parameters:\n",
    "        new_value (float): The new value from the time series.\n",
    "        \"\"\"\n",
    "        self.n += 1\n",
    "        delta = new_value - self.mean\n",
    "        self.mean += delta / self.n\n",
    "        delta2 = new_value - self.mean\n",
    "        self.m2 += delta * delta2\n",
    "\n",
    "        if self.n > 1:\n",
    "            self.std = np.sqrt(self.m2 / (self.n - 1))\n",
    "        else:\n",
    "            self.std = 0  # Avoid division by zero for the first point\n",
    "\n",
    "    def update(self, new_value):\n",
    "        \"\"\"\n",
    "        Detect whether a new value is an anomaly based on the Z-score.\n",
    "        \n",
    "        Parameters:\n",
    "        new_value (float): The new value from the time series.\n",
    "\n",
    "        Returns:\n",
    "        int: 1 if the value is an anomaly, 0 otherwise.\n",
    "        \"\"\"\n",
    "        if self.n < 2:\n",
    "            # Not enough data points to compute Z-score yet\n",
    "            self.anomalies.append(0)\n",
    "            self.update_stats(new_value)\n",
    "            return 0\n",
    "        \n",
    "        # Calculate Z-score for the new value\n",
    "        z_score = (new_value - self.mean) / self.std if self.std > 0 else 0\n",
    "        \n",
    "        # Check if the Z-score exceeds the threshold\n",
    "        if abs(z_score) > self.threshold:\n",
    "            anomaly = 1\n",
    "        else:\n",
    "            anomaly = 0\n",
    "        \n",
    "        # Update rolling statistics after checking for an anomaly\n",
    "        self.update_stats(new_value)\n",
    "        \n",
    "        # Append result to anomaly list\n",
    "        self.anomalies.append(anomaly)\n",
    "        \n",
    "        return anomaly\n",
    "\n",
    "\n",
    "def apply_omlad_streaming(df, threshold=3):\n",
    "    \"\"\"\n",
    "    Apply OMLADStreaming to a DataFrame, detect anomalies, and record time taken per point using time.perf_counter().\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with 'timestamp' and 'value' columns.\n",
    "    threshold (float): Z-score threshold for anomaly detection.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional columns 'predicted_anomaly' and 'time_taken_per_point'.\n",
    "    \"\"\"\n",
    "    omlad = OMLADStreaming(threshold=threshold)\n",
    "    \n",
    "    times_taken = []\n",
    "    anomalies = []\n",
    "    \n",
    "    for value in df['value']:\n",
    "        start_time = time.perf_counter()  # High precision timing\n",
    "        \n",
    "        # Detect anomaly\n",
    "        anomaly = omlad.update(value)\n",
    "        anomalies.append(anomaly)\n",
    "        \n",
    "        end_time = time.perf_counter()  # High precision timing\n",
    "        times_taken.append(end_time - start_time)\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    df['predicted_anomaly_OMLADStreaming'] = anomalies\n",
    "    df['time_taken_per_point_OMLADStreaming'] = times_taken\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OMLADEWMAStreaming:\n",
    "    def __init__(self, threshold=3, alpha=0.3):\n",
    "        \"\"\"\n",
    "        Streaming anomaly detection using EWMA for mean and standard deviation.\n",
    "        \n",
    "        Parameters:\n",
    "        threshold (float): Z-score threshold to classify anomalies.\n",
    "        alpha (float): Smoothing factor for EWMA. A higher value gives more weight to recent observations.\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.alpha = alpha\n",
    "        self.ewma_mean = None  # EWMA for mean\n",
    "        self.ewma_var = None   # EWMA for variance\n",
    "        self.std = None        # EWMA for standard deviation\n",
    "        self.anomalies = []    # To store anomaly detection results\n",
    "\n",
    "    def update_ewma(self, new_value):\n",
    "        \"\"\"\n",
    "        Update the EWMA mean and variance with a new value.\n",
    "        \n",
    "        Parameters:\n",
    "        new_value (float): The new value from the time series.\n",
    "        \"\"\"\n",
    "        if self.ewma_mean is None:\n",
    "            # Initialize EWMA mean and variance with the first value\n",
    "            self.ewma_mean = new_value\n",
    "            self.ewma_var = 0\n",
    "            self.std = 0\n",
    "        else:\n",
    "            # Update EWMA mean\n",
    "            self.ewma_mean = self.alpha * new_value + (1 - self.alpha) * self.ewma_mean\n",
    "\n",
    "            # Update EWMA variance (mean square error)\n",
    "            self.ewma_var = self.alpha * (new_value - self.ewma_mean)**2 + (1 - self.alpha) * self.ewma_var\n",
    "\n",
    "            # Calculate EWMA standard deviation\n",
    "            self.std = np.sqrt(self.ewma_var)\n",
    "\n",
    "    def update(self, new_value):\n",
    "        \"\"\"\n",
    "        Detect whether a new value is an anomaly based on the EWMA Z-score.\n",
    "        \n",
    "        Parameters:\n",
    "        new_value (float): The new value from the time series.\n",
    "\n",
    "        Returns:\n",
    "        int: 1 if the value is an anomaly, 0 otherwise.\n",
    "        \"\"\"\n",
    "        if self.std is None or self.std == 0:\n",
    "            # Not enough data to calculate standard deviation, can't detect anomalies\n",
    "            self.update_ewma(new_value)\n",
    "            self.anomalies.append(0)\n",
    "            return 0\n",
    "\n",
    "        # Calculate Z-score based on EWMA mean and standard deviation\n",
    "        z_score = (new_value - self.ewma_mean) / self.std\n",
    "        \n",
    "        # Check if the Z-score exceeds the threshold\n",
    "        if abs(z_score) > self.threshold:\n",
    "            anomaly = 1\n",
    "        else:\n",
    "            anomaly = 0\n",
    "\n",
    "        # Update EWMA stats after checking for an anomaly\n",
    "        self.update_ewma(new_value)\n",
    "\n",
    "        # Append result to anomaly list\n",
    "        self.anomalies.append(anomaly)\n",
    "        \n",
    "        return anomaly\n",
    "\n",
    "def apply_omladewma_streaming(df, threshold=3, alpha=0.3):\n",
    "    \"\"\"\n",
    "    Apply OMLADEWMAStreaming to a DataFrame, detect anomalies, and record time taken per point using time.perf_counter().\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with 'timestamp' and 'value' columns.\n",
    "    threshold (float): Z-score threshold for anomaly detection.\n",
    "    alpha (float): Smoothing factor for EWMA.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with an additional column 'predicted_anomaly' and 'time_taken_per_point_OMLADEWMAStreaming'.\n",
    "    \"\"\"\n",
    "    ewma = OMLADEWMAStreaming(threshold=threshold, alpha=alpha)\n",
    "    \n",
    "    times_taken = []\n",
    "    anomalies = []\n",
    "    \n",
    "    for value in df['value']:\n",
    "        start_time = time.perf_counter()  # High precision timing\n",
    "        \n",
    "        # Detect anomaly\n",
    "        anomaly = ewma.update(value)\n",
    "        anomalies.append(anomaly)\n",
    "        \n",
    "        end_time = time.perf_counter()  # High precision timing\n",
    "        times_taken.append(end_time - start_time)\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    df['predicted_anomaly_OMLADEWMAStreaming'] = anomalies\n",
    "    df['time_taken_per_point_OMLADEWMAStreaming'] = times_taken\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingHampelFilter:\n",
    "    def __init__(self, window_size=7, n_sigmas=3):\n",
    "        \"\"\"\n",
    "        Streaming Hampel Filter for anomaly detection.\n",
    "        \n",
    "        Parameters:\n",
    "        window_size (int): The size of the sliding window (number of points to include on each side).\n",
    "        n_sigmas (int): The number of median absolute deviations to use for the threshold.\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.n_sigmas = n_sigmas\n",
    "        self.window = []  # Sliding window to hold the recent data points\n",
    "\n",
    "    def update(self, new_value):\n",
    "        \"\"\"\n",
    "        Update the Hampel filter with a new value and detect if it's an anomaly.\n",
    "        \n",
    "        Parameters:\n",
    "        new_value (float): New value from the data stream.\n",
    "        \n",
    "        Returns:\n",
    "        int: 1 if the value is an anomaly, 0 otherwise.\n",
    "        \"\"\"\n",
    "        # Append the new value to the sliding window\n",
    "        self.window.append(new_value)\n",
    "\n",
    "        # Ensure the window size is maintained\n",
    "        if len(self.window) > 2 * self.window_size + 1:\n",
    "            self.window.pop(0)\n",
    "\n",
    "        # If the window is not full yet, return 0 (no anomaly)\n",
    "        if len(self.window) < 2 * self.window_size + 1:\n",
    "            return 0\n",
    "\n",
    "        # Calculate the median and MAD (Median Absolute Deviation) of the window\n",
    "        median = np.median(self.window)\n",
    "        mad = np.median(np.abs(self.window - median))\n",
    "\n",
    "        # Define the threshold based on the MAD\n",
    "        threshold = self.n_sigmas * mad\n",
    "\n",
    "        # Check if the current value is an anomaly\n",
    "        if np.abs(new_value - median) > threshold:\n",
    "            return 1  # Anomaly detected\n",
    "        else:\n",
    "            return 0  # No anomaly detected\n",
    "\n",
    "\n",
    "def apply_streaming_hampel_to_df(df, window_size=7, n_sigmas=3):\n",
    "    \"\"\"\n",
    "    Apply the Streaming Hampel filter to a time series data in a DataFrame and return a DataFrame with predicted anomalies and time taken per point.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with 'timestamp' and 'value' columns.\n",
    "    window_size (int): The size of the sliding window for anomaly detection.\n",
    "    n_sigmas (int): Number of MADs used for the threshold.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional columns 'predicted_anomaly_StreamingHampelFilter' and 'time_taken_per_point_StreamingHampelFilter'.\n",
    "    \"\"\"\n",
    "    hampel_filter = StreamingHampelFilter(window_size=window_size, n_sigmas=n_sigmas)\n",
    "\n",
    "    # Lists to store time taken and anomaly detection results\n",
    "    times_taken = []\n",
    "    anomalies = []\n",
    "    \n",
    "    for value in df['value']:\n",
    "        start_time = time.perf_counter()  # Start high-precision timing\n",
    "\n",
    "        # Detect anomaly\n",
    "        anomaly = hampel_filter.update(value)\n",
    "        anomalies.append(anomaly)\n",
    "\n",
    "        end_time = time.perf_counter()  # End high-precision timing\n",
    "        times_taken.append(end_time - start_time)\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    df['predicted_anomaly_StreamingHampelFilter'] = anomalies\n",
    "    df['time_taken_per_point_StreamingHampelFilter'] = times_taken\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADWIN:\n",
    "    def __init__(self, delta=0.001):\n",
    "        \"\"\"\n",
    "        ADWIN algorithm for adaptive windowing and change detection in data streams.\n",
    "\n",
    "        Parameters:\n",
    "        delta (float): Confidence level. Lower values make the algorithm more sensitive to changes.\n",
    "        \"\"\"\n",
    "        self.delta = delta\n",
    "        self.window = []\n",
    "        self.width = 0  # Size of the window\n",
    "        self.total = 0  # Sum of the elements in the window\n",
    "        self.total_squared = 0  # Sum of squares of the elements in the window\n",
    "\n",
    "    def _variance(self, mean):\n",
    "        \"\"\"\n",
    "        Calculate variance of the window.\n",
    "        \n",
    "        Parameters:\n",
    "        mean (float): The mean of the window.\n",
    "\n",
    "        Returns:\n",
    "        float: Variance of the window.\n",
    "        \"\"\"\n",
    "        if self.width == 0:\n",
    "            return 0\n",
    "        return (self.total_squared / self.width) - mean**2\n",
    "\n",
    "    def _cut_point(self, mean, mean_left, var_left, mean_right, var_right, n_left, n_right):\n",
    "        \"\"\"\n",
    "        Compute whether there is a significant difference between the two sub-windows.\n",
    "\n",
    "        Parameters:\n",
    "        mean (float): Mean of the entire window.\n",
    "        mean_left (float): Mean of the left sub-window.\n",
    "        mean_right (float): Mean of the right sub-window.\n",
    "        var_left (float): Variance of the left sub-window.\n",
    "        var_right (float): Variance of the right sub-window.\n",
    "        n_left (int): Number of points in the left sub-window.\n",
    "        n_right (int): Number of points in the right sub-window.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if change is detected, False otherwise.\n",
    "        \"\"\"\n",
    "        diff = abs(mean_left - mean_right)\n",
    "        epsilon = np.sqrt(\n",
    "            (1 / (2 * n_left)) * var_left +\n",
    "            (1 / (2 * n_right)) * var_right\n",
    "        ) + np.sqrt(self.delta / (2 * n_left)) + np.sqrt(self.delta / (2 * n_right))\n",
    "\n",
    "        return diff > epsilon\n",
    "\n",
    "    def update(self, value):\n",
    "        \"\"\"\n",
    "        Update the window with a new value and check for change points.\n",
    "\n",
    "        Parameters:\n",
    "        value (float): New value from the data stream.\n",
    "\n",
    "        Returns:\n",
    "        bool: True if a change is detected, False otherwise.\n",
    "        \"\"\"\n",
    "        # Add new value to the window\n",
    "        self.window.append(value)\n",
    "        self.width += 1\n",
    "        self.total += value\n",
    "        self.total_squared += value ** 2\n",
    "\n",
    "        mean = self.total / self.width\n",
    "        change_detected = False\n",
    "\n",
    "        for i in range(1, self.width):\n",
    "            left_window = self.window[:i]\n",
    "            right_window = self.window[i:]\n",
    "\n",
    "            # Calculate statistics for the left and right sub-windows\n",
    "            n_left, n_right = len(left_window), len(right_window)\n",
    "            mean_left = np.mean(left_window)\n",
    "            mean_right = np.mean(right_window)\n",
    "            var_left = np.var(left_window)\n",
    "            var_right = np.var(right_window)\n",
    "\n",
    "            # Check if a change occurred\n",
    "            if self._cut_point(mean, mean_left, var_left, mean_right, var_right, n_left, n_right):\n",
    "                # Change detected, shrink the window\n",
    "                self.window = self.window[i:]\n",
    "                self.width = len(self.window)\n",
    "                self.total = np.sum(self.window)\n",
    "                self.total_squared = np.sum(np.square(self.window))\n",
    "                change_detected = True\n",
    "                break\n",
    "\n",
    "        return change_detected\n",
    "\n",
    "def apply_adwin_to_df(df, delta=0.002):\n",
    "    \"\"\"\n",
    "    Apply ADWIN to a time series data in a DataFrame and return a DataFrame with predicted anomalies and time taken per point.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with 'timestamp' and 'value' columns.\n",
    "    delta (float): Confidence level for ADWIN.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional columns 'predicted_anomaly_ADWIN' and 'time_taken_per_point_ADWIN'.\n",
    "    \"\"\"\n",
    "    adwin = ADWIN(delta=delta)\n",
    "    \n",
    "    times_taken = []\n",
    "    anomalies = []\n",
    "    \n",
    "    for value in df['value']:\n",
    "        start_time = time.perf_counter()  # Start high-precision timing\n",
    "        \n",
    "        # Detect anomaly\n",
    "        anomaly = adwin.update(value)\n",
    "        anomalies.append(1 if anomaly else 0)\n",
    "        \n",
    "        end_time = time.perf_counter()  # End high-precision timing\n",
    "        times_taken.append(end_time - start_time)\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    df['predicted_anomaly_ADWIN'] = anomalies\n",
    "    df['time_taken_per_point_ADWIN'] = times_taken\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowDensityEstimation:\n",
    "    def __init__(self, window_size=50, bandwidth=1.0, density_threshold=0.01):\n",
    "        \"\"\"\n",
    "        Sliding Window Density Estimation for anomaly detection.\n",
    "        \n",
    "        Parameters:\n",
    "        window_size (int): Size of the sliding window to hold recent data points.\n",
    "        bandwidth (float): Bandwidth parameter for the Gaussian kernel.\n",
    "        density_threshold (float): Probability density threshold below which the point is considered an anomaly.\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.bandwidth = bandwidth\n",
    "        self.density_threshold = density_threshold\n",
    "        self.window = []\n",
    "\n",
    "    def gaussian_kernel(self, distance):\n",
    "        \"\"\"\n",
    "        Gaussian kernel function.\n",
    "        \n",
    "        Parameters:\n",
    "        distance (float): Distance between points.\n",
    "\n",
    "        Returns:\n",
    "        float: The Gaussian kernel value.\n",
    "        \"\"\"\n",
    "        return (1 / (np.sqrt(2 * np.pi) * self.bandwidth)) * np.exp(-0.5 * (distance / self.bandwidth) ** 2)\n",
    "\n",
    "    def kernel_density_estimate(self, value):\n",
    "        \"\"\"\n",
    "        Estimate the density of the value using a Gaussian kernel based on the sliding window.\n",
    "\n",
    "        Parameters:\n",
    "        value (float): The new value for which to estimate the density.\n",
    "\n",
    "        Returns:\n",
    "        float: The estimated density.\n",
    "        \"\"\"\n",
    "        distances = np.abs(np.array(self.window) - value)\n",
    "        kernel_values = self.gaussian_kernel(distances)\n",
    "        density = np.sum(kernel_values) / (len(self.window) * self.bandwidth)\n",
    "        return density\n",
    "\n",
    "    def update(self, value):\n",
    "        \"\"\"\n",
    "        Update the sliding window with a new value and detect anomalies using kernel density estimation.\n",
    "\n",
    "        Parameters:\n",
    "        value (float): New value from the data stream.\n",
    "\n",
    "        Returns:\n",
    "        int: 1 if the value is an anomaly, 0 otherwise.\n",
    "        \"\"\"\n",
    "        # Add the new value to the sliding window\n",
    "        self.window.append(value)\n",
    "\n",
    "        # If the window exceeds the window_size, remove the oldest value\n",
    "        if len(self.window) > self.window_size:\n",
    "            self.window.pop(0)\n",
    "\n",
    "        # If there are fewer points than needed for density estimation, return 0 (non-anomalous)\n",
    "        if len(self.window) < 2:\n",
    "            return 0\n",
    "\n",
    "        # Estimate the density of the new value\n",
    "        density = self.kernel_density_estimate(value)\n",
    "\n",
    "        # Detect anomaly based on the density threshold\n",
    "        if density < self.density_threshold:\n",
    "            return 1  # Anomaly detected\n",
    "        else:\n",
    "            return 0  # No anomaly detected\n",
    "\n",
    "def apply_streaming_swde_to_df(df, window_size=50, bandwidth=1.0, density_threshold=0.01):\n",
    "    \"\"\"\n",
    "    Apply Streaming Sliding Window Density Estimation to a time series data in a DataFrame \n",
    "    and return a DataFrame with predicted anomalies and time taken per point.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with 'timestamp' and 'value' columns.\n",
    "    window_size (int): Size of the sliding window for anomaly detection.\n",
    "    bandwidth (float): Bandwidth parameter for Gaussian kernel.\n",
    "    density_threshold (float): Probability density threshold for anomaly detection.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional columns 'predicted_anomaly_SWDE' and 'time_taken_per_point_SWDE'.\n",
    "    \"\"\"\n",
    "    swde = SlidingWindowDensityEstimation(window_size=window_size, bandwidth=bandwidth, density_threshold=density_threshold)\n",
    "\n",
    "    times_taken = []\n",
    "    anomalies = []\n",
    "    \n",
    "    for value in df['value']:\n",
    "        start_time = time.perf_counter()  # Start high-precision timing\n",
    "\n",
    "        # Detect anomaly\n",
    "        anomaly = swde.update(value)\n",
    "        anomalies.append(anomaly)\n",
    "\n",
    "        end_time = time.perf_counter()  # End high-precision timing\n",
    "        times_taken.append(end_time - start_time)\n",
    "    \n",
    "    # Add the results to the DataFrame\n",
    "    df['predicted_anomaly_SWDE'] = anomalies\n",
    "    df['time_taken_per_point_SWDE'] = times_taken\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWMA:\n",
    "    def __init__(self, alpha=0.3, threshold=3):\n",
    "        \"\"\"\n",
    "        Exponential Weighted Moving Average (EWMA) for anomaly detection.\n",
    "        \n",
    "        Parameters:\n",
    "        alpha (float): Smoothing factor for the EWMA, ranges between 0 and 1.\n",
    "        threshold (float): Z-score threshold for detecting anomalies.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha\n",
    "        self.threshold = threshold\n",
    "        self.ewma_value = None\n",
    "        self.ewma_std = None\n",
    "        self.n = 0  # Keeps track of the number of points processed\n",
    "        self.mean = 0\n",
    "        self.m2 = 0  # To track the variance\n",
    "\n",
    "    def update(self, value):\n",
    "        \"\"\"\n",
    "        Update the EWMA with a new value and detect anomalies.\n",
    "        \n",
    "        Parameters:\n",
    "        value (float): New value from the data stream.\n",
    "        \n",
    "        Returns:\n",
    "        int: 1 if the value is an anomaly, 0 otherwise.\n",
    "        \"\"\"\n",
    "        if self.ewma_value is None:\n",
    "            self.ewma_value = value\n",
    "            self.ewma_std = 0\n",
    "            return 0\n",
    "\n",
    "        # Update EWMA\n",
    "        self.ewma_value = self.alpha * value + (1 - self.alpha) * self.ewma_value\n",
    "\n",
    "        # Update the variance using Welford's online algorithm\n",
    "        self.n += 1\n",
    "        delta = value - self.mean\n",
    "        self.mean += delta / self.n\n",
    "        delta2 = value - self.mean\n",
    "        self.m2 += delta * delta2\n",
    "        variance = self.m2 / (self.n - 1) if self.n > 1 else 0\n",
    "        self.ewma_std = np.sqrt(variance)\n",
    "\n",
    "        # Calculate the Z-score\n",
    "        z_score = (value - self.ewma_value) / self.ewma_std if self.ewma_std > 0 else 0\n",
    "\n",
    "        # Detect anomaly\n",
    "        return 1 if np.abs(z_score) > self.threshold else 0\n",
    "\n",
    "def apply_ewma_to_df(df, alpha=0.3, threshold=3):\n",
    "    ewma = EWMA(alpha=alpha, threshold=threshold)\n",
    "\n",
    "    times_taken = []\n",
    "    anomalies = []\n",
    "\n",
    "    for value in df['value']:\n",
    "        start_time = time.perf_counter()\n",
    "        anomaly = ewma.update(value)\n",
    "        anomalies.append(anomaly)\n",
    "        end_time = time.perf_counter()\n",
    "        times_taken.append(end_time - start_time)\n",
    "\n",
    "    df['predicted_anomaly_EWMA'] = anomalies\n",
    "    df['time_taken_per_point_EWMA'] = times_taken\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankedSlidingWindowAnomalyDetection:\n",
    "    def __init__(self, window_size=50, rank_threshold=0.95):\n",
    "        \"\"\"\n",
    "        Ranked Sliding Window Anomaly Detection (RSWAD).\n",
    "        \n",
    "        Parameters:\n",
    "        window_size (int): Size of the sliding window to hold recent data points.\n",
    "        rank_threshold (float): Rank threshold for detecting anomalies (e.g., 0.95 means the top 5% ranked points are anomalies).\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.rank_threshold = rank_threshold\n",
    "        self.window = []\n",
    "\n",
    "    def update(self, value):\n",
    "        \"\"\"\n",
    "        Update the sliding window with a new value and detect anomalies using rank-based detection.\n",
    "        \n",
    "        Parameters:\n",
    "        value (float): New value from the data stream.\n",
    "        \n",
    "        Returns:\n",
    "        int: 1 if the value is an anomaly, 0 otherwise.\n",
    "        \"\"\"\n",
    "        self.window.append(value)\n",
    "\n",
    "        if len(self.window) > self.window_size:\n",
    "            self.window.pop(0)\n",
    "\n",
    "        if len(self.window) < self.window_size:\n",
    "            return 0\n",
    "\n",
    "        # Calculate median and deviations\n",
    "        median = np.median(self.window)\n",
    "        deviations = np.abs(np.array(self.window) - median)\n",
    "\n",
    "        # Rank the deviations\n",
    "        ranked_deviations = np.argsort(deviations)\n",
    "\n",
    "        # Rank the current value's deviation\n",
    "        current_deviation = np.abs(value - median)\n",
    "        current_rank = np.searchsorted(deviations[ranked_deviations], current_deviation)\n",
    "\n",
    "        # Calculate the normalized rank\n",
    "        normalized_rank = current_rank / len(self.window)\n",
    "\n",
    "        # Detect anomaly\n",
    "        return 1 if normalized_rank > self.rank_threshold else 0\n",
    "\n",
    "def apply_rswad_to_df(df, window_size=50, rank_threshold=0.95):\n",
    "    rswad = RankedSlidingWindowAnomalyDetection(window_size=window_size, rank_threshold=rank_threshold)\n",
    "\n",
    "    times_taken = []\n",
    "    anomalies = []\n",
    "\n",
    "    for value in df['value']:\n",
    "        start_time = time.perf_counter()\n",
    "        anomaly = rswad.update(value)\n",
    "        anomalies.append(anomaly)\n",
    "        end_time = time.perf_counter()\n",
    "        times_taken.append(end_time - start_time)\n",
    "\n",
    "    df['predicted_anomaly_RSWAD'] = anomalies\n",
    "    df['time_taken_per_point_RSWAD'] = times_taken\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingZScore:\n",
    "    def __init__(self, window_size=20, threshold=3):\n",
    "        \"\"\"\n",
    "        Moving Z-Score for anomaly detection.\n",
    "        \n",
    "        Parameters:\n",
    "        window_size (int): Size of the sliding window to hold recent data points.\n",
    "        threshold (float): Z-score threshold for detecting anomalies.\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        self.window = []\n",
    "\n",
    "    def update(self, value):\n",
    "        \"\"\"\n",
    "        Update the sliding window with a new value and detect anomalies using Z-Score.\n",
    "        \n",
    "        Parameters:\n",
    "        value (float): New value from the data stream.\n",
    "        \n",
    "        Returns:\n",
    "        int: 1 if the value is an anomaly, 0 otherwise.\n",
    "        \"\"\"\n",
    "        # Add the new value to the sliding window\n",
    "        self.window.append(value)\n",
    "\n",
    "        # If the window exceeds the window_size, remove the oldest value\n",
    "        if len(self.window) > self.window_size:\n",
    "            self.window.pop(0)\n",
    "\n",
    "        # If there are fewer points than the window size, return 0 (no anomaly detected)\n",
    "        if len(self.window) < self.window_size:\n",
    "            return 0\n",
    "\n",
    "        # Calculate mean and standard deviation of the window\n",
    "        mean = np.mean(self.window)\n",
    "        std = np.std(self.window)\n",
    "\n",
    "        # If the standard deviation is 0, treat the point as non-anomalous\n",
    "        if std == 0:\n",
    "            return 0\n",
    "\n",
    "        # Calculate the Z-score of the new value\n",
    "        z_score = (value - mean) / std\n",
    "\n",
    "        # Detect anomaly based on the Z-score threshold\n",
    "        if abs(z_score) > self.threshold:\n",
    "            return 1  # Anomaly detected\n",
    "        else:\n",
    "            return 0  # No anomaly detected\n",
    "\n",
    "def apply_moving_zscore_to_df(df, window_size=20, threshold=3):\n",
    "    \"\"\"\n",
    "    Apply Moving Z-Score to a time series data in a DataFrame and return a DataFrame with predicted anomalies and time taken per point.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with 'timestamp' and 'value' columns.\n",
    "    window_size (int): Size of the sliding window for anomaly detection.\n",
    "    threshold (float): Z-score threshold for anomaly detection.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with additional columns 'predicted_anomaly' (0: normal, 1: anomaly) and 'time_taken_per_point_MovingZScore'.\n",
    "    \"\"\"\n",
    "    mzs = MovingZScore(window_size=window_size, threshold=threshold)\n",
    "\n",
    "    times_taken = []\n",
    "    anomalies = []\n",
    "\n",
    "    for value in df['value']:\n",
    "        start_time = time.perf_counter()  # Start high-precision timing\n",
    "        anomaly = mzs.update(value)\n",
    "        anomalies.append(anomaly)\n",
    "        end_time = time.perf_counter()  # End high-precision timing\n",
    "        times_taken.append(end_time - start_time)\n",
    "\n",
    "    df['predicted_anomaly_MovingZScore'] = anomalies\n",
    "    df['time_taken_per_point_MovingZScore'] = times_taken\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dfs = []\n",
    "for i in range(len(dfs)):\n",
    "    df = dfs[i][1]\n",
    "    df = apply_omlad_streaming(df, threshold=3)\n",
    "    df = apply_omladewma_streaming(df, threshold=3, alpha=0.3)\n",
    "    df = apply_streaming_hampel_to_df(df, window_size=7, n_sigmas=3)\n",
    "    df = apply_adwin_to_df(df, delta=0.002)\n",
    "    df = apply_streaming_swde_to_df(df, window_size=50, bandwidth=1.0, density_threshold=0.01)\n",
    "    df = apply_ewma_to_df(df, alpha=0.3, threshold=3)\n",
    "    df = apply_rswad_to_df(df, window_size=50, rank_threshold=0.95)\n",
    "    df = apply_moving_zscore_to_df(df, window_size=20, threshold=3)\n",
    "    result_dfs.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Th concept of tolerance is crucial, as detecting the exact ground turth anomaly point could be challenging in a streamed enviroment. Hence, a 2% tolerance would consider detecting the anomaly within that range from the ground truth and award that in the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spn\\anaconda3\\envs\\final\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\spn\\anaconda3\\envs\\final\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\spn\\anaconda3\\envs\\final\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to apply 1% tolerance window and calculate metrics\n",
    "def apply_tolerance_and_evaluate(df, tolerance=0.01):\n",
    "    num_points = len(df)\n",
    "    tolerance_window = int(tolerance * num_points)  # Calculate the window size as 1% of the total points\n",
    "    \n",
    "    # Create a binary array for tolerance windows\n",
    "    tolerance_array = np.zeros(len(df), dtype=int)\n",
    "    anomaly_indices = df.index[df['anomaly'] == 1].tolist()\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        start = max(0, idx - tolerance_window)\n",
    "        end = min(num_points, idx + tolerance_window + 1)\n",
    "        tolerance_array[start:end] = 1  # Mark all points within the tolerance window as 'anomaly'\n",
    "    \n",
    "    # Add the tolerance window as a new column to the DataFrame\n",
    "    df['tolerance_window'] = tolerance_array\n",
    "    \n",
    "    methods = [col for col in df.columns if col.startswith('predicted_anomaly_')]\n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        precision = precision_score(df['tolerance_window'], df[method])\n",
    "        recall = recall_score(df['tolerance_window'], df[method])\n",
    "        f1 = f1_score(df['tolerance_window'], df[method])\n",
    "        \n",
    "        results[method] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to calculate average time taken per point for each method\n",
    "def calculate_average_time(df):\n",
    "    time_cols = [col for col in df.columns if col.startswith('time_taken_per_point_')]\n",
    "    avg_time_results = {}\n",
    "    \n",
    "    for time_col in time_cols:\n",
    "        avg_time = df[time_col].mean()\n",
    "        avg_time_results[time_col] = avg_time\n",
    "    \n",
    "    return avg_time_results\n",
    "\n",
    "final_results_list = []\n",
    "for ele in result_dfs:\n",
    "    # Apply the tolerance and evaluate metrics\n",
    "    metrics = apply_tolerance_and_evaluate(ele, tolerance=0.02)\n",
    "\n",
    "    # Calculate average time taken per point\n",
    "    average_times = calculate_average_time(ele)\n",
    "\n",
    "    # Convert metrics to a DataFrame\n",
    "    metrics_df = pd.DataFrame(metrics).T  # Transpose to have methods as rows\n",
    "\n",
    "    # Convert average times to a DataFrame\n",
    "    times_df = pd.DataFrame(list(average_times.items()), columns=['method', 'average_time'])\n",
    "\n",
    "    # Clean the 'method' column to match between both DataFrames\n",
    "    metrics_df = metrics_df.reset_index()  # Reset index to make method names a column\n",
    "    metrics_df['index'] = metrics_df['index'].apply(lambda x: x.split('_')[-1])  # Extract method names\n",
    "\n",
    "    times_df['method'] = times_df['method'].apply(lambda x: x.split('_')[-1])  # Extract method names\n",
    "\n",
    "    # Merge both DataFrames on the method name\n",
    "    final = pd.merge(metrics_df, times_df, left_on='index', right_on='method', how='inner')\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    final = final.drop(columns=['method'])\n",
    "\n",
    "    # append to final_results_list\n",
    "    final_results_list.append(final)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>average_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ADWIN</th>\n",
       "      <td>0.054949</td>\n",
       "      <td>0.822629</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.000451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EWMA</th>\n",
       "      <td>0.036809</td>\n",
       "      <td>0.018331</td>\n",
       "      <td>0.024366</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MovingZScore</th>\n",
       "      <td>0.054703</td>\n",
       "      <td>0.013978</td>\n",
       "      <td>0.020132</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMLADEWMAStreaming</th>\n",
       "      <td>0.048432</td>\n",
       "      <td>0.049748</td>\n",
       "      <td>0.047863</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMLADStreaming</th>\n",
       "      <td>0.204724</td>\n",
       "      <td>0.098451</td>\n",
       "      <td>0.119517</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RSWAD</th>\n",
       "      <td>0.080794</td>\n",
       "      <td>0.070311</td>\n",
       "      <td>0.062563</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SWDE</th>\n",
       "      <td>0.057087</td>\n",
       "      <td>0.024431</td>\n",
       "      <td>0.028602</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>StreamingHampelFilter</th>\n",
       "      <td>0.056354</td>\n",
       "      <td>0.124949</td>\n",
       "      <td>0.074400</td>\n",
       "      <td>0.000050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       precision    recall  f1_score  average_time\n",
       "index                                                             \n",
       "ADWIN                   0.054949  0.822629  0.098765      0.000451\n",
       "EWMA                    0.036809  0.018331  0.024366      0.000003\n",
       "MovingZScore            0.054703  0.013978  0.020132      0.000031\n",
       "OMLADEWMAStreaming      0.048432  0.049748  0.047863      0.000002\n",
       "OMLADStreaming          0.204724  0.098451  0.119517      0.000002\n",
       "RSWAD                   0.080794  0.070311  0.062563      0.000039\n",
       "SWDE                    0.057087  0.024431  0.028602      0.000015\n",
       "StreamingHampelFilter   0.056354  0.124949  0.074400      0.000050"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results_concat = pd.concat(final_results_list)\n",
    "\n",
    "# calculate the mean for each metric\n",
    "mean_metrics = final_results_concat.groupby('index').mean()\n",
    "\n",
    "mean_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ADWIN** achieved the highest recall (0.822629), indicating its effectiveness in identifying a large proportion of actual anomalies, but has low precision.\n",
    "- **OMLADStreaming** has the best precision (0.204724), meaning it has the highest ratio of true positives among the detected anomalies.\n",
    "- **OMLADStreaming** also has the highest F1 score (0.119517), reflecting a better balance between precision and recall compared to other methods.\n",
    "- All methods maintained low average processing times, suggesting efficient performance.\n",
    "- The nature of the datasets varies, which may affect the results.\n",
    "- No hyperparameter tuning was performed due to time constraints, which could lead to suboptimal performance of the algorithms.\n",
    "- The nature of the actual data is unknown, aside from having drift and seasonality, making this evaluation primarily a demonstration of the methods rather than definitive conclusions on their effectiveness.\n",
    "- This process serves to illustrate that further examination and analysis are essential for selecting the best anomaly detection algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
